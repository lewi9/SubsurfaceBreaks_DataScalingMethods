{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42660629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from statsmodels.stats.diagnostic import lilliefors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef7587",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"Broken_terrains_datasets\"\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a348fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random_state = np.random.RandomState(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01d880b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(INPUT_PATH)\n",
    "dfs = [pd.read_csv(os.path.join(INPUT_PATH, str(file) + \".txt\"), decimal='.', sep=';') for file in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c5eac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    dfs,\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22fb37a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df=df[\n",
    "    (df.X_C_Neighbor1!='undefined') \n",
    "    & (df.X_C_Neighbor2!='undefined')\n",
    "    & (df.X_C_Neighbor3!='undefined') \n",
    "    & (df.Z_N!=0) \n",
    "    & (df.n1_zn!=0) \n",
    "    & (df.n2_zn!=0) \n",
    "    & (df.n3_zn!=0) \n",
    "    & (df.DOC<0.90)  \n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e5a19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_n = ['EuclideanNeighbor1_N', 'EuclideanNeighbor2_N','EuclideanNeighbor3_N']\n",
    "euclidean_d = ['EuclideanNeighbor1_D', 'EuclideanNeighbor2_D','EuclideanNeighbor3_D']\n",
    "cosine_n = ['CosineNeighbor1_N', 'CosineNeighbor2_N','CosineNeighbor3_N']\n",
    "cosine_d = ['CosineNeighbor1_D', 'CosineNeighbor2_D','CosineNeighbor3_D']\n",
    "angle_n = ['AngleNeighbor1_N', 'AngleNeighbor2_N','AngleNeighbor3_N']\n",
    "angle_d = ['AngleNeighbor1_D', 'AngleNeighbor2_D','AngleNeighbor3_D']\n",
    "\n",
    "euclidean_n_sorted = ['Euclidean_N_Max', 'Euclidean_N_Min', 'Euclidean_N_Intermediate']\n",
    "euclidean_d_sorted = ['Euclidean_D_Max', 'Euclidean_D_Min', 'Euclidean_D_Intermediate']\n",
    "cosine_n_sorted = ['Cosine_N_Max', 'Cosine_N_Min', 'Cosine_N_Intermediate']\n",
    "cosine_d_sorted = ['Cosine_D_Max', 'Cosine_D_Min', 'Cosine_D_Intermediate']\n",
    "angle_n_sorted = ['Angle_N_Max', 'Angle_N_Min', 'Angle_N_Intermediate']\n",
    "angle_d_sorted = ['Angle_D_Max', 'Angle_D_Min', 'Angle_D_Intermediate']\n",
    "\n",
    "sorting_pairs = [\n",
    "    (euclidean_n, euclidean_n_sorted),\n",
    "    (euclidean_d, euclidean_d_sorted),\n",
    "    (cosine_n, cosine_n_sorted),\n",
    "    (cosine_d, cosine_d_sorted),\n",
    "    (angle_n, angle_n_sorted),\n",
    "    (angle_d, angle_d_sorted)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa26b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_values(row: pd.Series, output_columns: list) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Sort Neighbor values in descending order and return a Series with max, intermediate, and min values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        A pandas Series containing Neighbor values.\n",
    "    output_columns : list\n",
    "        A list of column names for the output Series.\n",
    "        Maximum value, intermediate value, minimum value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        A pandas Series with the maximum, intermediate, and minimum values.\n",
    "    \"\"\"\n",
    "    max_val = row.max()\n",
    "    min_val = row.min()\n",
    "    remaining_val = row.sum() - max_val - min_val\n",
    "    return pd.Series([max_val, min_val, remaining_val], index=output_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69cd62aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfs = [\n",
    "    filtered_df[list(cols)].apply(sort_values, axis=1, output_columns=list(sorted_cols))\n",
    "    for cols, sorted_cols in sorting_pairs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0152f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df=pd.concat([\n",
    "    filtered_df[['X_N']],\n",
    "    filtered_df[['Y_N']],\n",
    "    filtered_df[['Z_N']],\n",
    "    filtered_df[['X_D']],\n",
    "    filtered_df[['Y_D']],\n",
    "    filtered_df[['Z_D']],   \n",
    "    *sorted_dfs,\n",
    "    filtered_df[['File_number']],\n",
    "    filtered_df[['Fault']]   \n",
    "    ], \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46a4a380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: (132886, 26)\n",
      "class 1: (12411, 26)\n"
     ]
    }
   ],
   "source": [
    "df_for_downsampling = sorted_df.copy()\n",
    "class_count_0, class_count_1 = df_for_downsampling['Fault'].value_counts()\n",
    "class_0 = df_for_downsampling[df_for_downsampling['Fault'] == -1]\n",
    "class_1 = df_for_downsampling[df_for_downsampling['Fault'] == 1]# print the shape of the class\n",
    "print('class 0:', class_0.shape)\n",
    "print('class 1:', class_1.shape)\n",
    "\n",
    "class_0_under = class_0.sample(class_count_1, random_state=RANDOM_STATE)\n",
    "\n",
    "undersampled_df = pd.concat([class_0_under, class_1], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40d434d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mambo\\AppData\\Local\\Temp\\ipykernel_26760\\2204962863.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[y == -1] = 0  # Change labels from -1, 1 to 0, 1\n"
     ]
    }
   ],
   "source": [
    "X = undersampled_df.drop(columns=['Fault', 'File_number'])\n",
    "y = undersampled_df['Fault']\n",
    "y[y == -1] = 0  # Change labels from -1, 1 to 0, 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9bf88ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_normality(df: pd.DataFrame, alpha: float = 0.05) -> dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Check the normality of the data using:\n",
    "    - Lilliefors test\n",
    "    - Shapiro-Wilk test\n",
    "    - Jarque-Bera test\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing the data to be tested.\n",
    "    alpha : float, optional\n",
    "        Significance level for the tests (default is 0.05).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with the results of the normality tests.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        data = df[col].values\n",
    "        lilliefors_stat, lilliefors_p_value = lilliefors(data)\n",
    "        shapiro_stat, shapiro_p_value = stats.shapiro(data)\n",
    "        jarque_bera_stat, jarque_bera_p_value = stats.jarque_bera(data)\n",
    "\n",
    "        result[col] = {\n",
    "            \"Lilliefors stat\": lilliefors_stat,\n",
    "            \"Lilliefors p value\": lilliefors_p_value,\n",
    "            \"Lilliefors normal\": lilliefors_p_value > alpha,\n",
    "            \"Shapiro stat\": shapiro_stat,\n",
    "            \"Shapiro p value\": shapiro_p_value,\n",
    "            \"Shapiro normal\": shapiro_p_value > alpha,\n",
    "            \"Jarque-Bera stat\": jarque_bera_stat,\n",
    "            \"Jarque-Bera p value\": jarque_bera_p_value,\n",
    "            \"Jarque-Bera normal\": jarque_bera_p_value > alpha,\n",
    "        }\n",
    "    \n",
    "    result_df = pd.DataFrame(result).T\n",
    "    result_df = result_df.astype({\n",
    "        \"Lilliefors stat\": \"float\",\n",
    "        \"Lilliefors p value\": \"float\",\n",
    "        \"Lilliefors normal\": \"bool\",\n",
    "        \"Shapiro stat\": \"float\",\n",
    "        \"Shapiro p value\": \"float\",\n",
    "        \"Shapiro normal\": \"bool\",\n",
    "        \"Jarque-Bera stat\": \"float\",\n",
    "        \"Jarque-Bera p value\": \"float\",\n",
    "        \"Jarque-Bera normal\": \"bool\",\n",
    "    })\n",
    "    float_cols = result_df.select_dtypes(include=['float']).columns\n",
    "    result_df[float_cols] = result_df[float_cols].map(lambda x: f\"{x:.4f}\")\n",
    "    result_df.index.name = \"Feature\"\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2ddb11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"X_train\": pd.DataFrame(X_train, columns=undersampled_df.drop(columns=['Fault', 'File_number']).columns),\n",
    "    \"X_test\": pd.DataFrame(X_test, columns=undersampled_df.drop(columns=['Fault', 'File_number']).columns),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b018786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mambo\\.conda\\envs\\wszystko\\lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:586: UserWarning: scipy.stats.shapiro: For N > 5000, computed p-value may not be accurate. Current N is 18616.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "c:\\Users\\Mambo\\.conda\\envs\\wszystko\\lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:586: UserWarning: scipy.stats.shapiro: For N > 5000, computed p-value may not be accurate. Current N is 6206.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "normality_results = {}\n",
    "for name, dataset in datasets.items():\n",
    "    normality_results[name] = check_normality(dataset, alpha=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d9360925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_variance(df_1: pd.DataFrame, df_1_name: str, df_2: pd.DataFrame, df_2_name: str, alpha: float = 0.05) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Check the equality of variances between two DataFrames using:\n",
    "    - Brown-Forsythe test\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_1 : pd.DataFrame\n",
    "        The first input DataFrame.\n",
    "    df_1_name : str\n",
    "        The name of the first DataFrame.\n",
    "    df_2 : pd.DataFrame\n",
    "        The second input DataFrame.\n",
    "    df_2_name : str\n",
    "        The name of the second DataFrame.\n",
    "    alpha : float, optional\n",
    "        Significance level for the test (default is 0.05).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with the results of the variance equality tests.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    for col in df_1.columns:\n",
    "        data1 = df_1[col].values\n",
    "        data2 = df_2[col].values\n",
    "        brown_forsythe_stat, brown_forsythe_p_value = stats.levene(data1, data2, center=\"median\")\n",
    "\n",
    "        result[col] = {\n",
    "            f\"var {df_1_name}\": np.var(data1, ddof=1),\n",
    "            f\"var {df_2_name}\": np.var(data2, ddof=1),\n",
    "            f\"var diff {df_1_name} minus {df_2_name}\": np.var(data1, ddof=1) - np.var(data2, ddof=1),\n",
    "            \"Brown-Forsythe stat\": brown_forsythe_stat,\n",
    "            \"Brown-Forsythe p value\": brown_forsythe_p_value,\n",
    "            \"Brown-Forsythe equal variance\": brown_forsythe_p_value > alpha,\n",
    "        }\n",
    "\n",
    "    result_df = pd.DataFrame(result).T\n",
    "    result_df = result_df.astype({\n",
    "        f\"var {df_1_name}\": \"float\",\n",
    "        f\"var {df_2_name}\": \"float\",\n",
    "        f\"var diff {df_1_name} minus {df_2_name}\": \"float\",\n",
    "        \"Brown-Forsythe stat\": \"float\",\n",
    "        \"Brown-Forsythe p value\": \"float\",\n",
    "        \"Brown-Forsythe equal variance\": \"bool\",\n",
    "    })\n",
    "    float_cols = result_df.select_dtypes(include=['float']).columns\n",
    "    result_df[float_cols] = result_df[float_cols].map(lambda x: f\"{x:.4f}\")\n",
    "    result_df.index.name = \"Feature\"\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5e3cd056",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_variance = {\n",
    "    \"X train vs X test\": (datasets[\"X_train\"], \"X train\", datasets[\"X_test\"], \"X test\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "edf8c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_test_results = {}\n",
    "for name, (df1, df1_name, df2, df2_name) in datasets_variance.items():\n",
    "    variance_test_results[name] = check_variance(df1, df1_name, df2, df2_name, alpha=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6c62dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_means(df_1: pd.DataFrame, df_1_name: str, df_2: pd.DataFrame, df_2_name: str, alpha: float = 0.05) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Check the equality of means between two DataFrames using:\n",
    "    - Student's t-test\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_1 : pd.DataFrame\n",
    "        The first input DataFrame.\n",
    "    df_1_name : str\n",
    "        The name of the first DataFrame.\n",
    "    df_2 : pd.DataFrame\n",
    "        The second input DataFrame.\n",
    "    df_2_name : str\n",
    "        The name of the second DataFrame.\n",
    "    alpha : float, optional\n",
    "        Significance level for the tests (default is 0.05).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with the results of the mean equality tests.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    for col in df_1.columns:\n",
    "        data1 = df_1[col].values\n",
    "        data2 = df_2[col].values\n",
    "\n",
    "        t_stat, t_p_value = stats.ttest_ind(data1, data2, equal_var=True)\n",
    "\n",
    "        result[col] = {\n",
    "            f\"mean {df_1_name}\": np.mean(data1),\n",
    "            f\"mean {df_2_name}\": np.mean(data2),\n",
    "            f\"mean diff {df_1_name} minus {df_2_name}\": np.mean(data1) - np.mean(data2),\n",
    "            \"t stat\": t_stat,\n",
    "            \"t p value\": t_p_value,\n",
    "            \"equal means\": t_p_value > alpha,\n",
    "        }\n",
    "\n",
    "    result_df = pd.DataFrame(result).T\n",
    "    result_df = result_df.astype({\n",
    "        f\"mean {df_1_name}\": \"float\",\n",
    "        f\"mean {df_2_name}\": \"float\",\n",
    "        f\"mean diff {df_1_name} minus {df_2_name}\": \"float\",\n",
    "        \"t stat\": \"float\",\n",
    "        \"t p value\": \"float\",\n",
    "        \"equal means\": \"bool\",\n",
    "    })\n",
    "\n",
    "    float_cols = result_df.select_dtypes(include=['float']).columns\n",
    "    result_df[float_cols] = result_df[float_cols].map(lambda x: f\"{x:.4f}\")\n",
    "    result_df.index.name = \"Feature\"\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88269716",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test_results = {}\n",
    "for name, (df1, df1_name, df2, df2_name) in datasets_variance.items():\n",
    "    mean_test_results[name] = check_means(df1, df1_name, df2, df2_name, alpha=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd8304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_df_to_doc(doc: Document, df: pd.DataFrame, title: str):\n",
    "    \"\"\"\n",
    "    Add a heading and a table to the Word document based on the DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : Document\n",
    "        The Word document object.\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame to be added as a table.\n",
    "        The index name of the DataFrame will be used as the first column header.\n",
    "    title : str\n",
    "        The title for the table.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    doc.add_heading(title, level=2)\n",
    "    table = doc.add_table(rows=df.shape[0] + 1, cols=df.shape[1] + 1)\n",
    "    table.style = 'Table Grid'\n",
    "\n",
    "    hdr_cells = table.rows[0].cells\n",
    "    hdr_cells[0].text = df.index.name\n",
    "    \n",
    "    run = hdr_cells[0].paragraphs[0].runs[0]\n",
    "    run.font.bold = True\n",
    "\n",
    "    for i, col_name in enumerate(df.columns):\n",
    "        cell = hdr_cells[i + 1]\n",
    "        cell.text = str(col_name)\n",
    "        cell.paragraphs[0].runs[0].font.bold = True\n",
    "\n",
    "    for i, row_idx in enumerate(df.index):\n",
    "        row_cells = table.rows[i + 1].cells\n",
    "        row_cells[0].text = str(row_idx)\n",
    "\n",
    "        for j, col_name in enumerate(df.columns):\n",
    "            val = df.loc[row_idx, col_name]\n",
    "            row_cells[j + 1].text = str(val)\n",
    "\n",
    "    doc.add_page_break()\n",
    "\n",
    "tables_to_print = [\n",
    "    (f\"Normality Test Results (alpha={ALPHA}) - {name}\", normality_results[name])\n",
    "    for name in normality_results\n",
    "]\n",
    "\n",
    "tables_to_print += [\n",
    "    (f\"Variance Equality Test Results (alpha={ALPHA}) - {name}\", variance_test_results[name])\n",
    "    for name in variance_test_results\n",
    "]\n",
    "\n",
    "tables_to_print += [\n",
    "    (f\"Mean Equality Test Results (alpha={ALPHA}) - {name}\", mean_test_results[name])\n",
    "    for name in mean_test_results\n",
    "]\n",
    "\n",
    "doc = Document()\n",
    "doc.add_heading('Statistics', 0)\n",
    "\n",
    "for title, df in tables_to_print:\n",
    "    add_df_to_doc(doc, df, title)\n",
    "\n",
    "output_filename = 'Parameters.docx'\n",
    "doc.save(output_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wszystko",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
