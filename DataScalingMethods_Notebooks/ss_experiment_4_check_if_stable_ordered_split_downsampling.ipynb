{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c837d5-fb9d-4ce0-9fb5-d0b367d489df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import parallel_backend\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    f1_score, \n",
    "    precision_score, \n",
    "    recall_score,\n",
    ") \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"Broken_terrains_datasets\"\n",
    "#EXCLUDE_FILES = [\"KSH\", \"params\"]\n",
    "REAL_DATA_FILE = \"KSH_input_output_0.txt\"\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "SVM = \"SVM\"\n",
    "\n",
    "SS_MODE: Literal[1, 2, 3] = 2\n",
    "\n",
    "X_C_LOWER = 922000\n",
    "X_C_UPPER = 923500\n",
    "Y_C_LOWER = 249500\n",
    "Y_C_UPPER = 251100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b96b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SS_MODE != 2:\n",
    "    raise ValueError(\"Only SS_MODE == 2 is supported in this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29122103",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random_state = np.random.RandomState(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(INPUT_PATH)\n",
    "\n",
    "# dfs = [\n",
    "#     pd.read_csv(os.path.join(INPUT_PATH, file), decimal='.', sep=';') \n",
    "#     for file in files \n",
    "#     if file.endswith('.txt')\n",
    "#     and not any(exclude in file for exclude in EXCLUDE_FILES)\n",
    "# ]\n",
    "\n",
    "dfs = [pd.read_csv(os.path.join(INPUT_PATH, str(file) + \".txt\"), decimal='.', sep=';') for file in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = dfs[4]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "#ax.scatter(df_0['X_C'], df_0['Y_C'], df_0[\"Z_C\"])\n",
    "ax.scatter(df_0['X_C'], df_0['Y_C'], df_0[\"Z_C\"], c=df_0['Fault'], cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c244f6-b942-4bcc-bd12-08372ebb30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    dfs,\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b80d8c-899d-4331-9650-a6f612bbe899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa01cf19-5025-406c-ba65-2116cf06ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb339a8-bcb2-4929-ae2c-c65f51b8afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df=df[\n",
    "    (df.X_C_Neighbor1!='undefined') \n",
    "    & (df.X_C_Neighbor2!='undefined')\n",
    "    & (df.X_C_Neighbor3!='undefined') \n",
    "    & (df.Z_N!=0) \n",
    "    & (df.n1_zn!=0) \n",
    "    & (df.n2_zn!=0) \n",
    "    & (df.n3_zn!=0) \n",
    "    & (df.DOC<0.90)  \n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a9f3d5-5493-4769-9ae5-c27b8739fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d947f7-ad98-4e99-8e5e-936b803a9bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_n = ['EuclideanNeighbor1_N', 'EuclideanNeighbor2_N','EuclideanNeighbor3_N']\n",
    "euclidean_d = ['EuclideanNeighbor1_D', 'EuclideanNeighbor2_D','EuclideanNeighbor3_D']\n",
    "cosine_n = ['CosineNeighbor1_N', 'CosineNeighbor2_N','CosineNeighbor3_N']\n",
    "cosine_d = ['CosineNeighbor1_D', 'CosineNeighbor2_D','CosineNeighbor3_D']\n",
    "angle_n = ['AngleNeighbor1_N', 'AngleNeighbor2_N','AngleNeighbor3_N']\n",
    "angle_d = ['AngleNeighbor1_D', 'AngleNeighbor2_D','AngleNeighbor3_D']\n",
    "\n",
    "euclidean_n_sorted = ['Euclidean_N_Max', 'Euclidean_N_Min', 'Euclidean_N_Intermediate']\n",
    "euclidean_d_sorted = ['Euclidean_D_Max', 'Euclidean_D_Min', 'Euclidean_D_Intermediate']\n",
    "cosine_n_sorted = ['Cosine_N_Max', 'Cosine_N_Min', 'Cosine_N_Intermediate']\n",
    "cosine_d_sorted = ['Cosine_D_Max', 'Cosine_D_Min', 'Cosine_D_Intermediate']\n",
    "angle_n_sorted = ['Angle_N_Max', 'Angle_N_Min', 'Angle_N_Intermediate']\n",
    "angle_d_sorted = ['Angle_D_Max', 'Angle_D_Min', 'Angle_D_Intermediate']\n",
    "\n",
    "sorting_pairs = [\n",
    "    (euclidean_n, euclidean_n_sorted),\n",
    "    (euclidean_d, euclidean_d_sorted),\n",
    "    (cosine_n, cosine_n_sorted),\n",
    "    (cosine_d, cosine_d_sorted),\n",
    "    (angle_n, angle_n_sorted),\n",
    "    (angle_d, angle_d_sorted)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ec95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_values(row: pd.Series, output_columns: list) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Sort Neighbor values in descending order and return a Series with max, intermediate, and min values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        A pandas Series containing Neighbor values.\n",
    "    output_columns : list\n",
    "        A list of column names for the output Series.\n",
    "        Maximum value, intermediate value, minimum value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        A pandas Series with the maximum, intermediate, and minimum values.\n",
    "    \"\"\"\n",
    "    max_val = row.max()\n",
    "    min_val = row.min()\n",
    "    remaining_val = row.sum() - max_val - min_val\n",
    "    return pd.Series([max_val, min_val, remaining_val], index=output_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb84e93-f17f-4e08-a123-251b1e1bba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfs = [\n",
    "    filtered_df[list(cols)].apply(sort_values, axis=1, output_columns=list(sorted_cols))\n",
    "    for cols, sorted_cols in sorting_pairs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c0ac68-3718-4b5a-9eb6-a0397ec99c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df=pd.concat([\n",
    "    filtered_df[['X_N']],\n",
    "    filtered_df[['Y_N']],\n",
    "    filtered_df[['Z_N']],\n",
    "    filtered_df[['X_D']],\n",
    "    filtered_df[['Y_D']],\n",
    "    filtered_df[['Z_D']],   \n",
    "    *sorted_dfs,\n",
    "    filtered_df[['File_number']],\n",
    "    filtered_df[['Fault']]   \n",
    "    ], \n",
    "    axis=1\n",
    ")\n",
    "sorted_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca72c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SS_MODE == 2:\n",
    "    standardized_df = sorted_df.copy()\n",
    "    scaler_2_A = StandardScaler()\n",
    "    file_numbers = standardized_df[\"File_number\"].unique()\n",
    "\n",
    "    for file_number in file_numbers:\n",
    "        file_mask = standardized_df[\"File_number\"] == file_number\n",
    "        standardized_df.loc[\n",
    "            file_mask,\n",
    "            standardized_df.columns.difference([\"File_number\", \"Fault\"]),\n",
    "        ] = scaler_2_A.fit_transform(\n",
    "            standardized_df.loc[\n",
    "                file_mask,\n",
    "                standardized_df.columns.difference([\"File_number\", \"Fault\"]),\n",
    "            ]\n",
    "    )\n",
    "        \n",
    "    df_for_downsampling = standardized_df.copy()\n",
    "else:\n",
    "    df_for_downsampling = sorted_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file_number in df_for_downsampling[\"File_number\"].unique():\n",
    "    sub_df = df_for_downsampling[df_for_downsampling[\"File_number\"] == file_number]\n",
    "    try:\n",
    "        class_count_0, class_count_1 = sub_df['Fault'].value_counts()\n",
    "    except ValueError:\n",
    "        print(f\"Skipping file number {file_number} due to only one class present.\")\n",
    "        continue\n",
    "    class_0 = sub_df[sub_df['Fault'] == -1]\n",
    "    class_1 = sub_df[sub_df['Fault'] == 1]# print the shape of the class\n",
    "    print('class 0:', class_0.shape)\n",
    "    print('class 1:', class_1.shape)\n",
    "\n",
    "    class_0_under = class_0.sample(class_count_1, random_state=RANDOM_STATE)\n",
    "\n",
    "    dfs.append(pd.concat([class_0_under, class_1], axis=0, ignore_index=True))\n",
    "    \n",
    "undersampled_df = pd.concat(dfs, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b3d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_train_test_split(\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.Series,\n",
    "        test_size: float = 0.25,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and testing sets while preserving the order of samples.\n",
    "    It splits based on unique 'File_number' values to avoid data leakage.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Feature set.\n",
    "    y : pd.Series\n",
    "        Target labels.\n",
    "    test_size : float, optional\n",
    "        Proportion of the dataset to include in the test split (default is 0.25).\n",
    "    random_state : int, optional\n",
    "        Random seed for reproducibility (default is RANDOM_STATE).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    n_terrains = X['File_number'].nunique()\n",
    "    terrains = X['File_number'].unique()\n",
    "    train_size = int((1 - test_size) * n_terrains)\n",
    "    train_terrains = terrains[:train_size]\n",
    "    test_terrains = terrains[train_size:]\n",
    "\n",
    "    train_mask = X['File_number'].isin(train_terrains)\n",
    "    test_mask = X['File_number'].isin(test_terrains)\n",
    "\n",
    "    X_train = X[train_mask].drop(columns=['File_number'])\n",
    "    X_test = X[test_mask].drop(columns=['File_number'])\n",
    "    y_train = y[train_mask]\n",
    "    y_test = y[test_mask]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dbfe59-1dfb-43ac-b58c-36f2a64107ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = undersampled_df.drop(columns=['Fault'])\n",
    "y = undersampled_df['Fault']\n",
    "y[y == -1] = 0  # Change labels from -1, 1 to 0, 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = ordered_train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24bed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SS_MODE == 1:\n",
    "    scaler_1_A = StandardScaler()\n",
    "    X_train = scaler_1_A.fit_transform(X_train)\n",
    "    \n",
    "    scaler_1_B = StandardScaler()\n",
    "    X_test = scaler_1_B.fit_transform(X_test)\n",
    "\n",
    "if SS_MODE == 3:\n",
    "    scaler_3 = StandardScaler()\n",
    "    X_train = scaler_3.fit_transform(X_train)\n",
    "    X_test = scaler_3.transform(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b9170-50f4-4842-8674-e88b35521522",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "models[SVM] = SVC(kernel=\"rbf\", C=0.05, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c164d-669b-4cc1-8e8d-d78cdffe8d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, confusion, classification_rep, f1 = {}, {}, {}, {}, {}, {}\n",
    "\n",
    "for key in models.keys():\n",
    "    \n",
    "    # Fit the classifier\n",
    "    models[key].fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = models[key].predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy[key] = accuracy_score(predictions, y_test)\n",
    "    precision[key] = precision_score(predictions, y_test)\n",
    "    recall[key] = recall_score(predictions, y_test)\n",
    "    f1[key] = f1_score(predictions, y_test)\n",
    "    confusion[key]=confusion_matrix(predictions, y_test)\n",
    "    classification_rep[key]=classification_report(predictions, y_test, target_names=['homocline', 'Fault'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae0db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model: str) -> None:\n",
    "    print(f\"Metrics for {model}:\")\n",
    "    print(f\"Accuracy: {accuracy[model]:.4f}\")\n",
    "    print(f\"Precision: {precision[model]:.4f}\")\n",
    "    print(f\"Recall: {recall[model]:.4f}\")\n",
    "    print(f\"F1 Score: {f1[model]:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion[model]}\")\n",
    "    tn, fp, fn, tp = confusion[model].ravel()\n",
    "    print(f\"TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")\n",
    "    print(f\"Classification Report:\\n{classification_rep[model]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97998f-c660-47a6-97d5-93141097bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models.keys():\n",
    "    print_metrics(model)\n",
    "    print(\"================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ebacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model: ClassifierMixin, param_grid: dict, scoring: str, cv: int=5) -> ClassifierMixin:\n",
    "    \"\"\"\n",
    "    Perform grid search to find the best hyperparameters for a given model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : ClassifierMixin\n",
    "        The machine learning model to be optimized.\n",
    "    param_grid : dict\n",
    "        A dictionary where keys are hyperparameter names and values are lists of possible values.\n",
    "    scoring : str\n",
    "        The scoring metric to optimize (e.g., 'f1', 'accuracy').\n",
    "    cv : int, optional\n",
    "        The number of cross-validation folds (default is 5).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ClassifierMixin\n",
    "        The model with the best found hyperparameters.\n",
    "    \"\"\"\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring=scoring, cv=cv, n_jobs=-1, refit=True, verbose=True)\n",
    "    with parallel_backend('loky'):\n",
    "        with tqdm_joblib(tqdm(desc=\"GridSearch Progress\")):\n",
    "            grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for {model.__class__.__name__}: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "\n",
    "param_grid[SVM]= {\n",
    "    'C': [0.1, 1, 10, 100, 1000],  \n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "    'kernel': ['rbf', 'linear'],\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_tuning = {}\n",
    "\n",
    "#models_tuning[SVM] = grid_search(SVC(random_state=RANDOM_STATE), param_grid[SVM], scoring='f1')\n",
    "models_tuning[SVM] = SVC(kernel=\"rbf\", C=10, gamma=0.1, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79abe7-9639-4258-8234-07bc9fb70752",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tuning, precision_tuning, recall_tuning, confusion_tuning, classification_rep_tuning, f1_tuning = {}, {}, {}, {}, {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe05c3-67b3-49d8-bf8e-3e2aff33b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in models_tuning.keys():\n",
    "    \n",
    "    # Fit the classifier\n",
    "    models_tuning[key].fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions_tuning = models_tuning[key].predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy_tuning[key] = accuracy_score(predictions_tuning, y_test)\n",
    "    precision_tuning[key] = precision_score(predictions_tuning, y_test)\n",
    "    recall_tuning[key] = recall_score(predictions_tuning, y_test)\n",
    "    f1_tuning[key] = f1_score(predictions_tuning, y_test)\n",
    "    confusion_tuning[key]=confusion_matrix(predictions_tuning, y_test)\n",
    "    classification_rep_tuning[key]=classification_report(predictions_tuning, y_test, target_names=['homocline', 'Fault'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3466268-62ea-46a4-8952-50db2ab716ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_tuning(model: str) -> None:\n",
    "    print(f\"Metrics for {model} after tuning:\")\n",
    "    print(\"Model parameters:\", models_tuning[model].get_params())\n",
    "    print(f\"Accuracy: {accuracy_tuning[model]:.4f}\")\n",
    "    print(f\"Precision: {precision_tuning[model]:.4f}\")\n",
    "    print(f\"Recall: {recall_tuning[model]:.4f}\")\n",
    "    print(f\"F1 Score: {f1_tuning[model]:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_tuning[model]}\")\n",
    "    tn, fp, fn, tp = confusion_tuning[model].ravel()\n",
    "    print(f\"TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")\n",
    "    print(f\"Classification Report:\\n{classification_rep_tuning[model]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9799301",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_tuning.keys():\n",
    "    print_metrics_tuning(model)\n",
    "    print(\"================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4987095-7d06-4e95-a73e-c97715c3e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb36160-824f-4aeb-9cf4-80821d800173",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_raw_df = pd.read_csv(INPUT_PATH + \"/\" + REAL_DATA_FILE, decimal=\".\", sep=\";\")\n",
    "real_raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0328ff0e-0e80-4c2e-aef5-216ed4864d7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86549249",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd42af1-84cc-4459-87a3-18963efcd367",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_filtered_df=real_raw_df[\n",
    "    (real_raw_df.X_C_Neighbor1!='undefined') \n",
    "    & (real_raw_df.X_C_Neighbor2!='undefined')\n",
    "    & (real_raw_df.X_C_Neighbor3!='undefined') \n",
    "    & (real_raw_df.Z_N!=0) \n",
    "    & (real_raw_df.n1_zn!=0) \n",
    "    & (real_raw_df.n2_zn!=0) \n",
    "    & (real_raw_df.n3_zn!=0) \n",
    "    & (real_raw_df.DOC<0.90)  \n",
    "].reset_index(drop=True)\n",
    "real_filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db9d823-b7bf-4f33-803d-4514eeaa3398",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_sorted_dfs = [\n",
    "    real_filtered_df[list(cols)].apply(sort_values, axis=1, output_columns=list(sorted_cols))\n",
    "    for cols, sorted_cols in sorting_pairs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_sorted_df=pd.concat([\n",
    "    real_filtered_df[[\"X_C\",\"Y_C\",\"Z_C\"]],\n",
    "    real_filtered_df[['X_N']],\n",
    "    real_filtered_df[['Y_N']],\n",
    "    real_filtered_df[['Z_N']],\n",
    "    real_filtered_df[['X_D']],\n",
    "    real_filtered_df[['Y_D']],\n",
    "    real_filtered_df[['Z_D']],   \n",
    "    *real_sorted_dfs,\n",
    "    ], \n",
    "    axis=1\n",
    ")\n",
    "real_sorted_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d463491",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_X = real_sorted_df.drop(columns=[\"X_C\", \"Y_C\", \"Z_C\"])\n",
    "\n",
    "if SS_MODE == 1:\n",
    "    scaler_1_C = StandardScaler()\n",
    "    real_X = scaler_1_C.fit_transform(real_X)\n",
    "elif SS_MODE == 2:\n",
    "    scaler_2_B = StandardScaler()\n",
    "    real_X = scaler_2_B.fit_transform(real_X)\n",
    "elif SS_MODE == 3:\n",
    "    real_X = scaler_3.transform(real_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0696f5-f775-40fb-adc0-e544cecc4285",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_predictions = {\n",
    "    model: models_tuning[model].predict(real_X)\n",
    "    for model in models_tuning.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat(\n",
    "    [\n",
    "        real_sorted_df[[\"X_C\", \"Y_C\", \"Z_C\"]],\n",
    "        pd.DataFrame(real_predictions),\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2593e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.SVM.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_sorted_df_subset = real_sorted_df.query(f'{X_C_LOWER} <= X_C <= {X_C_UPPER} and {Y_C_LOWER} <= Y_C <= {Y_C_UPPER}')\n",
    "real_X_subset = real_sorted_df_subset.drop(columns=[\"X_C\", \"Y_C\", \"Z_C\"])\n",
    "\n",
    "if SS_MODE == 1:\n",
    "    scaler_1_C = StandardScaler()\n",
    "    real_X_subset  = scaler_1_C.fit_transform(real_X_subset )\n",
    "elif SS_MODE == 2:\n",
    "    scaler_2_B = StandardScaler()\n",
    "    real_X_subset  = scaler_2_B.fit_transform(real_X_subset )\n",
    "elif SS_MODE == 3:\n",
    "    real_X_subset  = scaler_3.transform(real_X_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaffe26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_predictions_subset = {\n",
    "    model: models_tuning[model].predict(real_X_subset)\n",
    "    for model in models_tuning.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_subset = pd.DataFrame(real_predictions_subset)\n",
    "preds_subset.index = real_sorted_df_subset.index\n",
    "\n",
    "results_df_subset = pd.concat(\n",
    "    [\n",
    "        real_sorted_df_subset[[\"X_C\", \"Y_C\", \"Z_C\"]],\n",
    "        preds_subset,\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "results_df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee5878",
   "metadata": {},
   "outputs": [],
   "source": [
    "inserted_df = results_df.copy()\n",
    "inserted_df.loc[results_df_subset.index, SVM] = results_df_subset[SVM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a106670",
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_df = results_df.copy()\n",
    "changes = results_df[SVM] != inserted_df[SVM]\n",
    "changes_df.loc[changes, SVM] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6dd901",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVM\n",
    "\n",
    "cmap = {0: 'aqua', 1: 'magenta', 2: 'yellow'}\n",
    "legend_labels_1 = ['homocline', 'fault']\n",
    "legend_labels_2 = ['homocline', 'fault', '\\'unstable\\'']\n",
    "legend_colors_1 = [mpatches.Patch(color=cmap[i], label=legend_labels_1[i]) for i in range(2)]\n",
    "legend_colors_2 = [mpatches.Patch(color=cmap[i], label=legend_labels_2[i]) for i in range(3)]\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(6.4*2, 4.8*2))\n",
    "\n",
    "axis = ax.ravel()\n",
    "\n",
    "axis[0].scatter(results_df[\"Y_C\"], results_df[\"X_C\"], s=2, color=results_df[model].map(cmap))\n",
    "axis[1].scatter(results_df_subset[\"Y_C\"], results_df_subset[\"X_C\"], s=2, color=results_df_subset[model].map(cmap))\n",
    "axis[2].scatter(inserted_df[\"Y_C\"], inserted_df[\"X_C\"], s=2, color=inserted_df[model].map(cmap))\n",
    "axis[3].scatter(changes_df[\"Y_C\"], changes_df[\"X_C\"], s=2, color=changes_df[model].map(cmap))\n",
    "\n",
    "axis[0].set_title(\"Predictions for the entire dataset\")\n",
    "axis[1].set_title(\"Predictions for the subset\")\n",
    "axis[2].set_title(\"Predictions for the entire dataset with inserted subset\")\n",
    "axis[3].set_title(\"Changes after inserting the subset\")\n",
    "\n",
    "for i in range(len(axis)):\n",
    "    axis[i].set_xlabel(\"Y_C\")\n",
    "    axis[i].set_ylabel(\"X_C\")\n",
    "    axis[i].tick_params(axis='x', rotation=45)\n",
    "    axis[i].axhline(y=X_C_LOWER, color='k', linestyle='--', linewidth=0.5)\n",
    "    axis[i].axhline(y=X_C_UPPER, color='k', linestyle='--', linewidth=0.5)\n",
    "    axis[i].axvline(x=Y_C_LOWER, color='k', linestyle='--', linewidth=0.5)\n",
    "    axis[i].axvline(x=Y_C_UPPER, color='k', linestyle='--', linewidth=0.5)\n",
    "    axis[i].legend(handles=legend_colors_1 if i != 3 else legend_colors_2, loc='upper right', fontsize='small', title='classes')\n",
    "\n",
    "fig.suptitle(f\"{model} predictions, StandardScaler method: {SS_MODE}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(f\"ordered_split_scaling_method_{SS_MODE}.svg\", format=\"svg\", dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d84e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVM\n",
    "\n",
    "cmap = {0: 'aqua', 1: 'magenta', 2: 'yellow'}\n",
    "legend_labels_1 = ['homocline', 'fault']\n",
    "legend_labels_2 = ['homocline', 'fault', '\\'unstable\\'']\n",
    "legend_colors_1 = [mpatches.Patch(color=cmap[i], label=legend_labels_1[i]) for i in range(2)]\n",
    "legend_colors_2 = [mpatches.Patch(color=cmap[i], label=legend_labels_2[i]) for i in range(3)]\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(6.4*2, 4.8*2))\n",
    "\n",
    "axis = ax.ravel()\n",
    "\n",
    "axis[0].scatter(results_df[\"Y_C\"], results_df[\"X_C\"], s=2, color=results_df[model].map(cmap))\n",
    "axis[1].scatter(results_df_subset[\"Y_C\"], results_df_subset[\"X_C\"], s=2, color=results_df_subset[model].map(cmap))\n",
    "axis[2].scatter(inserted_df[\"Y_C\"], inserted_df[\"X_C\"], s=2, color=inserted_df[model].map(cmap))\n",
    "axis[3].scatter(changes_df[\"Y_C\"], changes_df[\"X_C\"], s=2, color=changes_df[model].map(cmap))\n",
    "\n",
    "axis[0].set_title(\"Predictions for the entire dataset\")\n",
    "axis[1].set_title(\"Predictions for the subset\")\n",
    "axis[2].set_title(\"Predictions for the entire dataset with inserted subset\")\n",
    "axis[3].set_title(\"Changes after inserting the subset\")\n",
    "\n",
    "for i in range(len(axis)):\n",
    "    axis[i].set_xlabel(\"Y_C\")\n",
    "    axis[i].set_ylabel(\"X_C\")\n",
    "    axis[i].tick_params(axis='x', rotation=45)\n",
    "    axis[i].axhline(y=X_C_LOWER, color='k', linestyle='--', linewidth=0.5)\n",
    "    axis[i].axhline(y=X_C_UPPER, color='k', linestyle='--', linewidth=0.5)\n",
    "    axis[i].axvline(x=Y_C_LOWER, color='k', linestyle='--', linewidth=0.5)\n",
    "    axis[i].axvline(x=Y_C_UPPER, color='k', linestyle='--', linewidth=0.5)\n",
    "    axis[i].legend(handles=legend_colors_1 if i != 3 else legend_colors_2, loc='upper right', fontsize='small', title='classes')\n",
    "\n",
    "    rect = mpatches.Rectangle(\n",
    "        (Y_C_LOWER, X_C_LOWER),\n",
    "        Y_C_UPPER - Y_C_LOWER,\n",
    "        X_C_UPPER - X_C_LOWER,\n",
    "        linewidth=1.5,\n",
    "        edgecolor='r',\n",
    "        facecolor='none'\n",
    "    )\n",
    "    axis[i].add_patch(rect)\n",
    "\n",
    "\n",
    "center_y = (Y_C_LOWER + Y_C_UPPER) / 2\n",
    "center_x = (X_C_LOWER + X_C_UPPER) / 2\n",
    "\n",
    "ax_start = axis[1]\n",
    "ax_end = axis[2]\n",
    "\n",
    "arrow = mpatches.ConnectionPatch(\n",
    "    xyA=(center_y, center_x),\n",
    "    xyB=(center_y, center_x),\n",
    "    coordsA=\"data\",\n",
    "    coordsB=\"data\",\n",
    "    axesA=ax_start,\n",
    "    axesB=ax_end,\n",
    "    color=\"red\",\n",
    "    arrowstyle=\"->,head_length=4,head_width=3\",\n",
    "    linewidth=2\n",
    ")\n",
    "fig.add_artist(arrow)\n",
    "fig.suptitle(f\"{model} predictions, StandardScaler method: {SS_MODE}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(f\"ordered_split_scaling_method_{SS_MODE}_arrow.svg\", format=\"svg\", dpi=1200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wszystko",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
